{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6732cda-6bce-4a35-b056-2651d329b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import pickle  \n",
    "import os\n",
    "from pycaret.classification import load_model, predict_model\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9963a-dd79-48b6-9322-5e30b7e6a3a6",
   "metadata": {},
   "source": [
    "setup_function():\n",
    "- load out-of-sample data\n",
    "- load all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc9535-a008-44ae-9266-03328e0317c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading different datasets\n",
    "#df = pd.read_excel('/home/dsg/VORTEX_FINAL/PRODUCTION/DATA/real_world/real_world_data.xlsx',sheet_name='quiruelas', engine='openpyxl')\n",
    "#df = pd.read_excel('/home/dsg/VORTEX_FINAL/PRODUCTION/DATA/real_world/real_world_data.xlsx',sheet_name='v_higueras', engine='openpyxl')\n",
    "#df = pd.read_excel('/home/dsg/VORTEX_FINAL/PRODUCTION/DATA/real_world/real_world_data.xlsx',sheet_name='Alberite', engine='openpyxl')\n",
    "#df = pd.read_excel('/home/dsg/VORTEX_FINAL/PRODUCTION/DATA/real_world/real_world_data.xlsx',sheet_name='Paternanbidea', engine='openpyxl')\n",
    "#df = pd.read_excel('/home/dsg/VORTEX_FINAL/PRODUCTION/DATA/real_world/real_world_data.xlsx',sheet_name='Can_Gambus', engine='openpyxl')\n",
    "#df = pd.read_excel('/home/dsg/VORTEX_FINAL/PRODUCTION/DATA/real_world/real_world_data.xlsx',sheet_name='CatalonianSites', engine='openpyxl')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe070084-d417-4ab8-8990-24c9a7e068b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading models\n",
    "full_model = load_model(model_path)\n",
    "destilled_model = load_model(model_path)\n",
    "#specific\n",
    "VdH = load_model(model_path)\n",
    "PQ = load_model(model_path)\n",
    "French = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468168c9-8a45-4d15-b1ab-59473f9df843",
   "metadata": {},
   "source": [
    "### Prediction_function():\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49edbc-2cd4-4d2d-aa1d-035ebfa120ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction(model_path, excel_path, exclude_columns=None, output_column='predictions'):\n",
    "    \"\"\"\n",
    "    Carga un archivo Excel,\n",
    "    hace predicciones usando un modelo pre-entrenado,\n",
    "    y guarda los resultados en un nuevo archivo Excel o CSV.\n",
    "\n",
    "    :param model_path: Ruta al archivo del modelo \n",
    "    :param excel_path: Ruta al archivo con los datos de entrada\n",
    "    :param exclude_columns: Lista de columnas a excluir como entrada (opcional)\n",
    "    :param output_column: Nombre de la columna para las predicciones (por defecto: 'predictions')\n",
    "    :return: Ruta al archivo de salida con las predicciones\n",
    "    \"\"\"\n",
    "    # Loading  the model usando PyCaret\n",
    "    modelo = load_model(model_path)\n",
    "\n",
    "    # Cargar los datos\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "\n",
    "    # Excluir columnas si se especifican\n",
    "    if exclude_columns:\n",
    "        df = df.drop(columns=[col for col in exclude_columns], errors='ignore')\n",
    "\n",
    "    # Hacer predicciones usando PyCaret\n",
    "    predictions = predict_model(model, data=df,raw_score=True)\n",
    "    \n",
    "    # El resultado de predict_model ya incluye las predicciones, \n",
    "    # generalmente en una columna llamada 'Label' o similar\n",
    "    # Renombrar la columna de predicciones si es necesario\n",
    "    if 'Label' in predicciones.columns:\n",
    "        predicciones = predicciones.rename(columns={'Label': output_column})\n",
    "    elif 'prediction_label' in predicciones.columns:\n",
    "        predicciones = predicciones.rename(columns={'prediction_label': output_column})\n",
    "    else:\n",
    "        # Si la columna de predicciones tiene otro nombre, ajusta esto según sea necesario\n",
    "        pass\n",
    "\n",
    "   \n",
    "    # Create name for the output file\n",
    "    base_name = os.path.splitext(excel_path)[0]\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    output_path = f\"{base_name}_{current_date}_predictions.xlsx\"\n",
    "   \n",
    "\n",
    "    # save results\n",
    "    predictions.to_excel(output_path, index=False)\n",
    "    predictions.to_csv(output_path, index=False)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a773dee-c50b-4f04-863f-35a9424ae9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage:\n",
    "model_path = '20240827_SinCaDestilledVortex2SMOTECalibrated'\n",
    "excel_path = \"./DATA/VARISCITA ROMANA100.xlsx\"\n",
    "exclude_columns = [] \n",
    "\n",
    "try:\n",
    "    output_file = predict_from_excel(model_path, excel_path, exclude_columns)\n",
    "    print(f\"Predictions saved in: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46397d4-aa6e-481e-93f2-2f656f54d75e",
   "metadata": {},
   "source": [
    "\n",
    "class ProvenanceAndUncertaintyAnalysis:\n",
    "\n",
    "- def predicitons_uncertainty_analysis():\n",
    "  - Average confidence per site\n",
    "  - Median entropy per site\n",
    "  - Median probabilities per site\n",
    "\n",
    "- Provenance determination\n",
    "  - Confidence treshold >0.7\n",
    "  - Majority vote sourcing\n",
    "  - Homogeneity\n",
    " \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a6529-bbb4-425b-bcf7-501014c47f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def predictions_uncertainty_analysis(prediction_df, confidence_threshold=0.7):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Performs uncertainty analysis per site\n",
    "\n",
    "    Args:\n",
    "        prediction_df: df with probabilistic predictions\n",
    "        confidence_treshold: threshold to mark the prediction as uncertain\n",
    "     \n",
    "    \"\"\"\n",
    "    # Convert score columns to numbers\n",
    "    score_cols = ['prediction_score_CT', 'prediction_score_PCM', 'prediction_score_PDLC']\n",
    "    for col in score_cols:\n",
    "        df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Obtener probabilidades\n",
    "    probas = df[score_cols].values\n",
    "    \n",
    "    # Obtener predicción y confianza\n",
    "    predictions = df['predicciones'].values\n",
    "    confidences = np.max(probas, axis=1)\n",
    "    \n",
    "    # Marcar predicciones por debajo del umbral como inciertas\n",
    "    uncertain_mask = confidences < confidence_threshold\n",
    "    predictions_with_uncertainty = predictions.copy()\n",
    "    predictions_with_uncertainty[uncertain_mask] = 'uncertain'\n",
    "    \n",
    "    # Calcular entropía \n",
    "    entropies = np.array([entropy(probs, base=2) for probs in probas])\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'Site': df['Site'],\n",
    "        'Original_predictions': predictions,\n",
    "        'Uncertainty_treshold_predictions': predictions_with_uncertainty,\n",
    "        'entropy': entropies\n",
    "    })\n",
    "    \n",
    "    # Imprimir métricas básicas\n",
    "    n_uncertain = np.sum(uncertain_mask)\n",
    "    print(f\"Uncertain predictions: {n_uncertain}/{len(df)} ({(n_uncertain/len(df)*100):.1f}%)\")\n",
    "    print(f\"Median entropies: {entropies.median():.3f}\")\n",
    "    \n",
    "    \n",
    "    # Calcular y mostrar la mediana de entropia por sitio\n",
    "    print(\"\\nMedian entropy per site:\")\n",
    "    entropy_median_by_site = results_df.groupby('Site')['entropy'].median()\n",
    "    for site, median in entropy_median_by_site.items():\n",
    "        print(f\"{site}: {median:.3f}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f76c2-f826-4fdf-8f33-0df876314563",
   "metadata": {},
   "source": [
    "Plotting function:\n",
    "- Create uncertain plot\n",
    "- Save it in plots\n",
    "- close the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457b1de-0434-4eee-bb69-65c9b9aaaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
